{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "new_directory = \"E:/subject/compulsory_elective_2/real_project/\"\n",
    "os.chdir(new_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = \"https://api.smith.langchain.com\"\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': {'input_question': \"What is the purpose of the 'Making the Most of Every Bite' cookbook?\"},\n",
       " 'output': 'It is designed to provide high-protein, high-calorie recipes for patients experiencing weight loss due to illness.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('testing/cooking/cooking.json', 'r') as file:\n",
    "    dataset = json.load(file)\n",
    "    \n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What is the purpose of the 'Making the Most of Every Bite' cookbook?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['input']['input_question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#format the dataset before create dataset in Langsmith\n",
    "def format_dataset(dataset):\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    \n",
    "    for example in dataset:\n",
    "        inputs.append({\"question\": example['input']['input_question']})\n",
    "        outputs.append({\"answer\": example['output']})\n",
    "\n",
    "        \n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, outputs = format_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': \"What is the purpose of the 'Making the Most of Every Bite' cookbook?\"},\n",
       " {'question': 'Who endorsed the contents of the cookbook?'},\n",
       " {'question': 'What is a common ingredient used to enrich milk in the recipes?'},\n",
       " {'question': 'What type of soups does the cookbook emphasize for patients with difficulty swallowing?'},\n",
       " {'question': 'What is the recommendation for consuming fluids during nausea?'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset in Langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "dataset_name = 'Cooking Testing Dataset'\n",
    "\n",
    "dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=inputs,\n",
    "    outputs=outputs,\n",
    "    dataset_id=dataset.id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create chatbot for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup retriever\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorsdb = FAISS.load_local(\n",
    "    'store/cooking', embeddings=embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = vectorsdb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up Q&A chain\n",
    "llm = ChatOpenAI(model='gpt-4o')\n",
    "\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "#create Q&A chain\n",
    "document_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "#create history aware chain\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "#create retrieval chain\n",
    "retrieval_chain = create_retrieval_chain(history_aware_retriever, document_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The purpose of the 'Making the Most of Every Bite' cookbook is to provide high protein, high calorie recipes specifically designed for patients and their carers.\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieval_chain.invoke(\n",
    "    {\"input\": \"What is the purpose of the 'Making the Most of Every Bite' cookbook?\"}\n",
    ")\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rag_answer(example: dict):\n",
    "    \"\"\"Use this for answer evaluation\"\"\"\n",
    "    response = retrieval_chain.invoke({\"input\": example['question']})\n",
    "    return {\"answer\": response[\"answer\"]}\n",
    "\n",
    "def predict_rag_answer_with_context(example: dict):\n",
    "    \"\"\"Use this for evaluation of retrieved documents and hallucinations\"\"\"\n",
    "    response = retrieval_chain.invoke({\"input\": example['question']})\n",
    "    return {\"answer\": response[\"answer\"], \"context\": response[\"context\"]}\n",
    "\n",
    "def accuracy(results):\n",
    "    return sum(results) / len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### response vs reference answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Grade prompt\n",
    "grade_prompt_answer_accuracy = prompt = hub.pull(\"langchain-ai/rag-answer-vs-reference\")\n",
    "\n",
    "res_answer_evaluator = []\n",
    "\n",
    "def answer_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer accuracy\n",
    "    \"\"\"\n",
    "    # Get question, ground truth answer, RAG chain answer\n",
    "    input_question = example.inputs['question']\n",
    "    reference = example.outputs['answer']\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_answer_accuracy | llm\n",
    "\n",
    "    # Run evaluator\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"correct_answer\": reference,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "    \n",
    "    res_answer_evaluator.append(score)\n",
    "\n",
    "    return {\"key\": \"answer_v_reference_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'cooking-bdbd7969' at:\n",
      "https://smith.langchain.com/o/e0a07099-8389-43f1-bdb8-a03361a3989c/datasets/f243d5d8-a90c-471f-adc0-f929a3170fea/compare?selectedSessions=a4a7cb60-929e-4c50-a34a-b66addd80d7d\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:06,  1.63it/s]\n"
     ]
    }
   ],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_evaluator],\n",
    "    experiment_prefix=\"cooking\",\n",
    "    metadata={\"version\": \"LCEL context, gpt-4o\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(res_answer_evaluator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response vs input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade prompt\n",
    "grade_prompt_answer_helpfulness = prompt = hub.pull(\"langchain-ai/rag-answer-helpfulness\")\n",
    "\n",
    "res_answer_helpfulness_evaluator = []\n",
    "\n",
    "def answer_helpfulness_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for RAG answer helpfulness\n",
    "    \"\"\"\n",
    "\n",
    "    # Get question, ground truth answer, RAG chain answer\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_answer_helpfulness | llm\n",
    "\n",
    "    # Run evaluator\n",
    "    score = answer_grader.invoke({\"question\": input_question,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "    res_answer_helpfulness_evaluator.append(score)\n",
    "\n",
    "    return {\"key\": \"answer_helpfulness_score\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-answer-helpfulness-65d2655e' at:\n",
      "https://smith.langchain.com/o/e0a07099-8389-43f1-bdb8-a03361a3989c/datasets/f243d5d8-a90c-471f-adc0-f929a3170fea/compare?selectedSessions=e2f527f2-9594-4e49-9eda-534a72cb9d50\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:07,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "experiment_results = evaluate(\n",
    "    predict_rag_answer,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_helpfulness_evaluator],\n",
    "    experiment_prefix=\"rag-answer-helpfulness\",\n",
    "    metadata={\"version\": \"LCEL context, gpt-4o\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(res_answer_helpfulness_evaluator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response vs retrieved docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "grade_prompt_hallucinations = prompt = hub.pull(\"langchain-ai/rag-answer-hallucination\")\n",
    "\n",
    "res_answer_hallucination_evaluator = []\n",
    "\n",
    "def answer_hallucination_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for generation hallucination\n",
    "    \"\"\"\n",
    "\n",
    "    # RAG inputs\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    contexts = run.outputs[\"context\"]\n",
    "\n",
    "    # RAG answer\n",
    "    prediction = run.outputs[\"answer\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_hallucinations | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"documents\": contexts,\n",
    "                                  \"student_answer\": prediction})\n",
    "    score = score[\"Score\"]\n",
    "    res_answer_hallucination_evaluator.append(score)\n",
    "\n",
    "    return {\"key\": \"answer_hallucination\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-answer-hallucination-ff4b81bc' at:\n",
      "https://smith.langchain.com/o/e0a07099-8389-43f1-bdb8-a03361a3989c/datasets/f243d5d8-a90c-471f-adc0-f929a3170fea/compare?selectedSessions=c5525fc6-8351-438b-8df8-5ddfc14b15e6\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:15,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_hallucination_evaluator],\n",
    "    experiment_prefix=\"rag-answer-hallucination\",\n",
    "    metadata={\"version\": \"LCEL context, gpt-4o\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(res_answer_hallucination_evaluator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieved docs vs input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade prompt\n",
    "grade_prompt_doc_relevance = hub.pull(\"langchain-ai/rag-document-relevance\")\n",
    "\n",
    "res_docs_relevance_evaluator = []\n",
    "\n",
    "def docs_relevance_evaluator(run, example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator for document relevance\n",
    "    \"\"\"\n",
    "\n",
    "    # RAG inputs\n",
    "    input_question = example.inputs[\"question\"]\n",
    "    contexts = run.outputs[\"context\"]\n",
    "\n",
    "    # LLM grader\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "    # Structured prompt\n",
    "    answer_grader = grade_prompt_doc_relevance | llm\n",
    "\n",
    "    # Get score\n",
    "    score = answer_grader.invoke({\"question\":input_question,\n",
    "                                  \"documents\":contexts})\n",
    "    score = score[\"Score\"]\n",
    "    res_docs_relevance_evaluator.append(score)\n",
    "\n",
    "    return {\"key\": \"document_relevance\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'rag-doc-relevance-0d578206' at:\n",
      "https://smith.langchain.com/o/e0a07099-8389-43f1-bdb8-a03361a3989c/datasets/f243d5d8-a90c-471f-adc0-f929a3170fea/compare?selectedSessions=4bc1f5a0-a9ca-4af6-98c8-dedb35c40c2c\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MhJPWKOvQLyUJ5zKxURHvcpt on tokens per min (TPM): Limit 30000, Used 29184, Requested 841. Please try again in 50ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run a56e0e74-d15b-47ce-b708-ad8f68f5f334: KeyError('context')\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Hi There\\AppData\\Local\\Temp\\ipykernel_41104\\4129342952.py\", line 13, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"context\"]\n",
      "               ~~~~~~~~~~~^^^^^^^^^^^\n",
      "KeyError: 'context'\n",
      "1it [00:02,  2.24s/it]Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MhJPWKOvQLyUJ5zKxURHvcpt on tokens per min (TPM): Limit 30000, Used 29704, Requested 1251. Please try again in 1.91s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run 820c000a-6133-4d2b-8058-a4b7173733f6: KeyError('context')\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Hi There\\AppData\\Local\\Temp\\ipykernel_41104\\4129342952.py\", line 13, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"context\"]\n",
      "               ~~~~~~~~~~~^^^^^^^^^^^\n",
      "KeyError: 'context'\n",
      "2it [00:03,  1.43s/it]Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run bbdb4d6b-e5f7-44b9-8f90-5865c0ddb056: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MhJPWKOvQLyUJ5zKxURHvcpt on tokens per min (TPM): Limit 30000, Used 29403, Requested 1275. Please try again in 1.356s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Hi There\\AppData\\Local\\Temp\\ipykernel_41104\\4129342952.py\", line 22, in docs_relevance_evaluator\n",
      "    score = answer_grader.invoke({\"question\":input_question,\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3024, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5354, in invoke\n",
      "    return self.bound.invoke(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 851, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 683, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 742, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1277, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 954, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1043, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1092, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1043, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1092, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1058, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MhJPWKOvQLyUJ5zKxURHvcpt on tokens per min (TPM): Limit 30000, Used 29403, Requested 1275. Please try again in 1.356s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MhJPWKOvQLyUJ5zKxURHvcpt on tokens per min (TPM): Limit 30000, Used 29248, Requested 1456. Please try again in 1.408s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "3it [00:05,  1.99s/it]Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run 6b0c5576-19f3-4345-8169-a46df3456964: KeyError('context')\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Hi There\\AppData\\Local\\Temp\\ipykernel_41104\\4129342952.py\", line 13, in docs_relevance_evaluator\n",
      "    contexts = run.outputs[\"context\"]\n",
      "               ~~~~~~~~~~~^^^^^^^^^^^\n",
      "KeyError: 'context'\n",
      "Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run e01f53ed-5602-4053-8667-8025901e1b9d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MhJPWKOvQLyUJ5zKxURHvcpt on tokens per min (TPM): Limit 30000, Used 29707, Requested 1607. Please try again in 2.628s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Hi There\\AppData\\Local\\Temp\\ipykernel_41104\\4129342952.py\", line 22, in docs_relevance_evaluator\n",
      "    score = answer_grader.invoke({\"question\":input_question,\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3024, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5354, in invoke\n",
      "    return self.bound.invoke(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 851, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 683, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 742, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1277, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 954, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1043, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1092, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1043, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1092, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1058, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MhJPWKOvQLyUJ5zKxURHvcpt on tokens per min (TPM): Limit 30000, Used 29707, Requested 1607. Please try again in 2.628s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "5it [00:07,  1.44s/it]Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run ecf14da4-abc9-408e-93f6-fdb062be8291: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MhJPWKOvQLyUJ5zKxURHvcpt on tokens per min (TPM): Limit 30000, Used 29617, Requested 1665. Please try again in 2.564s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Hi There\\AppData\\Local\\Temp\\ipykernel_41104\\4129342952.py\", line 22, in docs_relevance_evaluator\n",
      "    score = answer_grader.invoke({\"question\":input_question,\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3024, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5354, in invoke\n",
      "    return self.bound.invoke(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 851, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 683, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 742, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1277, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 954, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1043, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1092, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1043, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1092, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1058, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MhJPWKOvQLyUJ5zKxURHvcpt on tokens per min (TPM): Limit 30000, Used 29617, Requested 1665. Please try again in 2.564s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "6it [00:07,  1.07s/it]Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run bd7d90b8-7875-4c23-ac90-b66695599847: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MhJPWKOvQLyUJ5zKxURHvcpt on tokens per min (TPM): Limit 30000, Used 29065, Requested 2235. Please try again in 2.6s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Hi There\\AppData\\Local\\Temp\\ipykernel_41104\\4129342952.py\", line 22, in docs_relevance_evaluator\n",
      "    score = answer_grader.invoke({\"question\":input_question,\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3024, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5354, in invoke\n",
      "    return self.bound.invoke(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 851, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 683, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 742, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1277, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 954, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1043, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1092, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1043, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1092, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1058, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MhJPWKOvQLyUJ5zKxURHvcpt on tokens per min (TPM): Limit 30000, Used 29065, Requested 2235. Please try again in 2.6s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "8it [00:10,  1.09s/it]Error running evaluator <DynamicRunEvaluator docs_relevance_evaluator> on run 9126ee6a-3f8c-4d41-8348-eef93cbe7d05: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MhJPWKOvQLyUJ5zKxURHvcpt on tokens per min (TPM): Limit 30000, Used 28320, Requested 1803. Please try again in 246ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\evaluation\\_runner.py\", line 1357, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\evaluation\\evaluator.py\", line 327, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 612, in wrapper\n",
      "    raise e\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langsmith\\run_helpers.py\", line 609, in wrapper\n",
      "    function_result = run_container[\"context\"].run(func, *args, **kwargs)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Hi There\\AppData\\Local\\Temp\\ipykernel_41104\\4129342952.py\", line 22, in docs_relevance_evaluator\n",
      "    score = answer_grader.invoke({\"question\":input_question,\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 3024, in invoke\n",
      "    input = context.run(step.invoke, input, config)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py\", line 5354, in invoke\n",
      "    return self.bound.invoke(\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 286, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 786, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 643, in generate\n",
      "    raise e\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 633, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 851, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 683, in _generate\n",
      "    response = self.client.create(**payload)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 274, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 742, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1277, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 954, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1043, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1092, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1043, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1092, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"e:\\subject\\compulsory_elective_2\\real_project\\venv\\Lib\\site-packages\\openai\\_base_client.py\", line 1058, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o in organization org-MhJPWKOvQLyUJ5zKxURHvcpt on tokens per min (TPM): Limit 30000, Used 28320, Requested 1803. Please try again in 246ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "10it [00:14,  1.41s/it]\n"
     ]
    }
   ],
   "source": [
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[docs_relevance_evaluator],\n",
    "    experiment_prefix=\"rag-doc-relevance\",\n",
    "    metadata={\"version\": \"LCEL context, gpt-4o\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(res_docs_relevance_evaluator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
